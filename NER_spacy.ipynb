{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_spacy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/vivekster12/Parser/blob/master/NER_spacy.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "9J_ItoDGx6lU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "THIS IS FROM SPACY WEBSITE/ TUTORIAL"
      ]
    },
    {
      "metadata": {
        "id": "_y9JiSbJ6OQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "#import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxf96Ks7G4vK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FaKb4zrZ6ebz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "64d1df74-9f1d-4351-e595-8ce2e5e13c7f"
      },
      "cell_type": "code",
      "source": [
        "!pip install pathlib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pathlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathlib\n",
            "  Running setup.py bdist_wheel for pathlib ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f9/b2/4a/68efdfe5093638a9918bd1bb734af625526e849487200aa171\n",
            "Successfully built pathlib\n",
            "Installing collected packages: pathlib\n",
            "Successfully installed pathlib-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pOM-9_0R6koa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        },
        "outputId": "27f6ee81-49f2-411f-8f18-b3346ab9dd90"
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Using cached https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.5)\n",
            "Collecting murmurhash<0.29,>=0.28 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz\n",
            "Collecting cymem<1.32,>=1.30 (from spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/9e/273fbea507de99166c11cd0cb3fde1ac01b5bc724d9a407a2f927ede91a1/cymem-1.31.2.tar.gz\n",
            "Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/be/fc/09684555ce0ee7086675e6be698e4efeb6d9b315fd5aa96bed347572282b/preshed-1.0.1.tar.gz\n",
            "Collecting thinc<6.11.0,>=6.10.3 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz\n",
            "Collecting plac<1.0.0,>=0.9.6 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting ujson>=1.35 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz\n",
            "Collecting dill<0.3,>=0.2 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz\n",
            "Collecting regex==2017.4.5 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Collecting msgpack<1.0.0,>=0.5.6 (from thinc<6.11.0,>=6.10.3->spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 7.3MB/s \n",
            "\u001b[?25hCollecting msgpack-numpy<1.0.0,>=0.4.1 (from thinc<6.11.0,>=6.10.3->spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/84/09/fc890664a7a1dd0a88f46c93fb9340d0a27a69e82095a4a54aef2ed94a6d/msgpack_numpy-0.4.3.1-py2.py3-none-any.whl\n",
            "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz\n",
            "Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
            "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.11.0,>=6.10.3->spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Building wheels for collected packages: spacy, murmurhash, cymem, preshed, thinc, ujson, dill, regex, cytoolz, wrapt\n",
            "  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/60/0b/bb/7c2e28db574dbb2358176934eddd32a1c5f838ba0bc23eaaab\n",
            "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b8/94/a4/f69f8664cdc1098603df44771b7fec5fd1b3d8364cdd83f512\n",
            "  Running setup.py bdist_wheel for cymem ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/55/8d/4a/f6328252aa2aaec0b1cb906fd96a1566d77f0f67701071ad13\n",
            "  Running setup.py bdist_wheel for preshed ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ca/e5/8b/73706d7232da301838e0bc564367a2f7b2fc8f834228fc8a4b\n",
            "  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n",
            "  Running setup.py bdist_wheel for ujson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/e2/5d/17/f87cb7751896ac629b435a8696f83ee75b11029f5d6f6bda72\n",
            "  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n",
            "  Running setup.py bdist_wheel for wrapt ... \u001b[?25l-\b \b\\"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \bdone\r\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\r\n",
            "Successfully built spacy murmurhash cymem preshed thinc ujson dill regex cytoolz wrapt\n",
            "Installing collected packages: murmurhash, cymem, preshed, msgpack, msgpack-numpy, cytoolz, wrapt, plac, tqdm, dill, thinc, ujson, regex, spacy\n",
            "Successfully installed cymem-1.31.2 cytoolz-0.9.0.1 dill-0.2.8.2 msgpack-0.5.6 msgpack-numpy-0.4.3.1 murmurhash-0.28.0 plac-0.9.6 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 tqdm-4.24.0 ujson-1.35 wrapt-1.10.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8VYhksz674Tw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training data\n",
        "TRAIN_DATA = [\n",
        "    ('Who is Shaka Khan?', {\n",
        "        'entities': [(7, 17, 'PERSON')]\n",
        "    }),\n",
        "    ('Who is Ghengis Khan?', {\n",
        "        'entities': [(7, 17, 'PERSON')]\n",
        "    }),\n",
        "    ('Who is chingis Khan?', {\n",
        "        'entities': [(7, 17, 'PERSON')]\n",
        "    }),\n",
        "    ('I like London and Berlin.', {\n",
        "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
        "    }),\n",
        "\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_laZduEI1-y3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c55ce186-7a77-45e6-c08a-0062653af193"
      },
      "cell_type": "code",
      "source": [
        "print (type(TRAIN_DATA[0][1]))\n",
        "# print (TRAIN_DATA[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1HGu10-mUo0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "c6702d53-9a2c-43bd-a4cf-78f1e3dcb900"
      },
      "cell_type": "code",
      "source": [
        "f = open('test', 'wb')\n",
        "f.write('message')\n",
        "files.download('test')\n",
        "f.close()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7f7ef18b1605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mAvYcx4G8kht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def main(model=None, output_dir=None, n_iter=10):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank('en')  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "        print ('there')\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        print ('everywehre')\n",
        "        ner = nlp.get_pipe('ner')\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        print (type(annotations))\n",
        "        for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "    \n",
        " \n",
        "    \n",
        "  \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "                print (type(text))\n",
        "                print (type(annotations))\n",
        "            print(losses)\n",
        "\n",
        "    # test the trained model\n",
        "    for text, _ in TRAIN_DATA:\n",
        "        doc = nlp(text)\n",
        "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is None:\n",
        "        print ('here')\n",
        "        output_dir = Path('Libraries\\Documents')\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        x = nlp.to_bytes()\n",
        "        return x\n",
        "#         print (type(x))\n",
        "#         f = open('test', 'wb')\n",
        "#         f.write(x)\n",
        "#         files.download('test')\n",
        "#         f.close()\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        for text in TEST_DATA:\n",
        "            print (text, '***')\n",
        "            doc = nlp2(text)\n",
        "            print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AoPAKT1DbJE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "b52b5bdd-2477-415d-acef-c2bb9f95861c"
      },
      "cell_type": "code",
      "source": [
        "  # test the saved model\n",
        "  print(\"Loading from\", output_dir)\n",
        "  \n",
        "  nlp2 = spacy.blank('en')\n",
        "  ner = nlp2.create_pipe('ner')\n",
        "  nlp2.add_pipe(ner, last=True)\n",
        "  nlp2 = nlp2.from_bytes(x)\n",
        "  for text in TEST_DATA:\n",
        "      print (text, '***')\n",
        "      doc = nlp2(text)\n",
        "      print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "      print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from Libraries\\Documents\n",
            "Who is afzal khan? ***\n",
            "Entities [('afzal khan', 'PERSON')]\n",
            "Tokens [('Who', '', 2), ('is', '', 2), ('afzal', 'PERSON', 3), ('khan', 'PERSON', 1), ('?', '', 2)]\n",
            "I like Montreal. ***\n",
            "Entities [('Montreal', 'LOC')]\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('Montreal', 'LOC', 3), ('.', '', 2)]\n",
            "I do not like Paris. ***\n",
            "Entities [('Paris', 'LOC')]\n",
            "Tokens [('I', '', 2), ('do', '', 2), ('not', '', 2), ('like', '', 2), ('Paris', 'LOC', 3), ('.', '', 2)]\n",
            "I have never met Mr.Khan. ***\n",
            "Entities [('never met', 'PERSON')]\n",
            "Tokens [('I', '', 2), ('have', '', 2), ('never', 'PERSON', 3), ('met', 'PERSON', 1), ('Mr', '', 2), ('.', '', 2), ('Khan', '', 2), ('.', '', 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UwZB_xP1_7kV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "669fec91-d31f-4d2f-dde3-d85d4b4fea14"
      },
      "cell_type": "code",
      "source": [
        "x = main()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "there\n",
            "<class 'dict'>\n",
            "<class 'dict'>\n",
            "<class 'dict'>\n",
            "<class 'dict'>\n",
            "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "{'ner': 19.01908772374736}\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "{'ner': 19.26106513489099}\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n",
            "<class 'str'>\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-8f387f8d1628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-7036d3f6d449>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dropout - make it harder to memorise data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# callable to update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     losses=losses)\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_gold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_golds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "S9QuUTX0CZ0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "TEST_DATA = [\n",
        "    ('Who is afzal khan?'),\n",
        "    ('I like Montreal.'),\n",
        "    ('I do not like Paris.'),\n",
        "    ('I have never met Mr.Khan.')\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}